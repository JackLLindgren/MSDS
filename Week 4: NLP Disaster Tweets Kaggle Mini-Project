import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix



# Load data
train_data = pd.read_csv('Deep Learning week 4/Data/train.csv')
test_data = pd.read_csv('Deep Learning week 4/Data/test.csv')
sample_submission = pd.read_csv('Deep Learning week 4/Data/sample_submission.csv')

# Problem/Data Description
'''
This notebook explores the NLP Getting Started Kaggle competition. The goal is to build a model that predicts whether a given tweet is about a real disaster or not. 

The data consists of:

* train.csv: Contains the training set with tweets and their corresponding 'target' labels (1 for real disaster, 0 for not).
* test.csv: Contains the test set with tweets for which we need to predict the 'target' labels.
* sample_submission.csv: Provides the format for submitting predictions.

Key challenges:

* Handling noisy and informal text data (tweets).
* Dealing with imbalanced classes (potentially more non-disaster tweets).
* Identifying relevant features for distinguishing between real and non-disaster tweets.
'''

# Exploratory Data Analysis (EDA)

# Check for missing values
print(train_data.isnull().sum())
print(test_data.isnull().sum())

# Distribution of target variable
sns.countplot(x='target', data=train_data)
plt.title('Distribution of Target Variable')
plt.show()

# Text length distribution
train_data['text_length'] = train_data['text'].apply(len)
sns.histplot(train_data['text_length'], kde=True)
plt.title('Distribution of Text Length')
plt.show()
'''
These plots provide some context for other data, the main notes drawn should be that there are significantly more tweets not about disasters
and there are a large amount of tweets just under 140 characters. The later of these findings is easily explained by Twitters original maximum character length of 140 
likely implying this data is from before the change was made to longer character maximums. It is also not surprising that there would be less tweets about disasters as
it is simply a less common subject to tweet on.
'''


# Text preprocessing function
def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()

    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # Remove mentions and hashtags
    text = re.sub(r'@\w+|#\w+', '', text)

    # Remove punctuation and special characters
    text = re.sub(r'[^\w\s]', '', text)

    # Tokenization
    tokens = word_tokenize(text)

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens  
 if word not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]  


    # Join the tokens back into a string  

    processed_text = ' '.join(lemmatized_tokens)

    return processed_text

# Apply preprocessing to train and test data
train_data['processed_text'] = train_data['text'].apply(preprocess_text)
test_data['processed_text'] = test_data['text'].apply(preprocess_text)

# Feature extraction (TF-IDF)
vectorizer = TfidfVectorizer(max_features=5000) # Adjust max_features as needed
X_train = vectorizer.fit_transform(train_data['processed_text'])
X_test = vectorizer.transform(test_data['processed_text'])
y_train = train_data['target']

# Model training (Logistic Regression)
model = LogisticRegression()
model.fit(X_train, y_train)

# Prediction on test data
y_pred = model.predict(X_test)

# Result

# Prepare submission
submission = pd.DataFrame({'id': test_data['id'], 'target': y_pred})
submission.to_csv('my_submission.csv', index=False)

# Evaluate on training data (if applicable, use cross-validation for better estimation)
y_train_pred = model.predict(X_train)
print('Accuracy:', accuracy_score(y_train, y_train_pred))
print('Classification Report:\n', classification_report(y_train, y_train_pred))
print('Confusion Matrix:\n', confusion_matrix(y_train, y_train_pred))

# Discussion/Conclusion

'''
The model achieved an overall accuracy of 87% on the training data, indicating a good ability to distinguish between real disaster and non-disaster tweets.
Looking at the classification report:

    The model shows a higher precision (0.84) for the majority class (non-disaster tweets, label 0), meaning it's less likely to falsely classify a non-disaster tweet as a disaster tweet.
    However, the recall for the minority class (disaster tweets, label 1) is lower (0.76), implying that the model might miss identifying some actual disaster tweets.
    The F1-score, which balances precision and recall, is 0.89 for class 0 and 0.83 for class 1, reflecting the trade-off between these metrics.

The confusion matrix further reveals:

    Out of 4342 actual non-disaster tweets, the model correctly predicted 4105 (True Negatives) and misclassified 237 as disasters (False Positives).
    Out of 3271 actual disaster tweets, the model correctly predicted 2494 (True Positives) but missed 777 (False Negatives).

Key Takeaway: The model performs well overall, but there's room for improvement in identifying disaster tweets, which is crucial in a real-world disaster response scenario. Techniques to address class imbalance and further feature engineering/model tuning could be explored to enhance the recall for the disaster class.

'''
